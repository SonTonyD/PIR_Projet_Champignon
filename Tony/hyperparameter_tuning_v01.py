# -*- coding: utf-8 -*-
"""Hyperparameter_tuning_v01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W3qYDqhGW_bXsdomREX7bACMro_YPGru
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Clear any logs from previous runs
!rm -rf ./logs/

!pip install split-folders

# Commented out IPython magic to ensure Python compatibility.
# %rm -rf sample_data

# Commented out IPython magic to ensure Python compatibility.
# %rm -rf saved_model

# Commented out IPython magic to ensure Python compatibility.
# %rm -rf Mushrooms

import splitfolders
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorboard.plugins.hparams import api as hp
from PIL import ImageFile

import numpy as np
import pandas as pd
import seaborn as sn
import shutil




ImageFile.LOAD_TRUNCATED_IMAGES = True

rmdir /content/Mushrooms/input_dataset/.ipynb_checkpoints

"""**Paramètres à tuner**"""

HP_ALPHA = hp.HParam('alpha_lrs', hp.Discrete([0.005 , 0.01 , 0.05 , 0.1, 0.5, 1.0 ,5.0]))
HP_LR = hp.HParam('lr', hp.Discrete([ 0.00005 ,0.0001,0.0005 ,0.001, 0.005, 0.01, 0.05 ]))

METRIC_ACCURACY = 'accuracy'

with tf.summary.create_file_writer('logs/hparam_tuning').as_default():
  hp.hparams_config(
    hparams=[HP_ALPHA, HP_LR],
    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],
  )

"""**Mettre les données**



"""

def show_matrix(model, test_generator):

    filenames = test_generator.filenames
    nb_samples = len(test_generator)
    y_prob=[]
    y_act=[]
    test_generator.reset()
    for _ in range(nb_samples):
        X_test, Y_test = test_generator.next()
        y_prob.append(model.predict(X_test))
        y_act.append(Y_test)

    predicted_class = [list(test_generator.class_indices.keys())[i.argmax()] for i in y_prob]
    actual_class = [list(test_generator.class_indices.keys())[i.argmax()] for i in y_act]

    out_df = pd.DataFrame(np.vstack([predicted_class, actual_class]).T, columns=["predicted_class", "actual_class"])
    confusion_matrix = pd.crosstab(out_df["actual_class"], out_df["predicted_class"], rownames=["Actual"], colnames=["Predicted"], normalize="columns")

    sn.heatmap(confusion_matrix, cmap="Blues", annot=True, fmt=".2f")
    print('test accuracy : {}'.format((np.diagonal(confusion_matrix).sum()/confusion_matrix.sum().sum()*100)))
    return (np.diagonal(confusion_matrix).sum()/confusion_matrix.sum().sum()*100)

def split_datas(ratio, seed):
    """ This function allows to split input data in 3 differents dir : train/test/val according the ratio
        @ratio (0.6, 0.2, 0.2)
    """

    input_folder = "drive/MyDrive/Mushrooms/input_dataset"
    output_folder = "Mushrooms/processed_Data"
    splitfolders.ratio(input_folder, output_folder, seed=seed, ratio=ratio)

def train_test_model(hparams, session_num):

  if (session_num != 0):
    shutil.rmtree('Mushrooms')

  split_datas((0.75, 0.15, 0.1), 65+session_num)

  IMG_HEIGHT, IMG_WIDTH = (224, 224)  # height width
  BATCH_SIZE = 32  # nombre d'image processed en meme temps  (https://arxiv.org/pdf/1404.5997.pdf)
  TRAIN_DIR = r"Mushrooms/processed_Data/train"
  VALIDATION_DIR = r"Mushrooms/processed_Data/val"
  TEST_DIR = r"Mushrooms/processed_Data/test"
  EPOCHS = 20
  pretrain = "inception_v3"
  preprocess_input = getattr(__import__("tensorflow.keras.applications."+pretrain, fromlist=["preprocess_input"]), "preprocess_input")
  model_name = pretrain+"/e"+str(EPOCHS)+"_b"+str(BATCH_SIZE)+"_boolean_bolete"
  print("Save model to : saved_model/{}.h5".format(model_name))

  train_datagen = ImageDataGenerator(
      preprocessing_function=preprocess_input,  # preprocess input from RESNET50, convert RGB to BGR
      shear_range=0.2,  # image will be distorted along an axis
      zoom_range=0.2,  # zoom de l'image
      horizontal_flip=True,  # allow horizontal flip
      vertical_flip=True,  # allow vertical flip
      validation_split=0.2  # Float. Fraction of images reserved for validation (strictly between 0 and 1).
  )

  train_generator = train_datagen.flow_from_directory(
      TRAIN_DIR,
      target_size=(IMG_HEIGHT, IMG_WIDTH),
      batch_size=BATCH_SIZE,
      class_mode="categorical",
      subset="training"
  )

  valid_generator = train_datagen.flow_from_directory(
      VALIDATION_DIR,
      target_size=(IMG_HEIGHT, IMG_WIDTH),
      batch_size=BATCH_SIZE,
      class_mode="categorical",
      subset="validation"
  )
  test_generator = train_datagen.flow_from_directory(
      TEST_DIR,
      target_size=(IMG_HEIGHT, IMG_WIDTH),
      batch_size=1,
      shuffle=True,
      class_mode="categorical",
      subset="validation"
  )





  def scheduler(epoch, lr):
    if epoch < 10:
        return lr
    else:
        return lr * tf.math.exp(-hparams[HP_ALPHA])

  base_model = InceptionV3(include_top=False, weights='imagenet')
  for layer in base_model.layers:
    layer.trainable = False  # layer from ResNet network won't be trained

  model = Sequential()
  model.add(Sequential(
    [
      layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),  # flip vertical ou horizontal
      layers.experimental.preprocessing.RandomRotation(0.2),  # range : [-20% * 2pi, 20% * 2pi].
      layers.experimental.preprocessing.RandomZoom(0.1),  # random zoom

    ]
  ))
  model.add(base_model)  # We don't use fully connected layer
  model.add(Flatten())  # Need to flatten the cnn
  model.add(Dropout(0.3))
  model.add(Dense(2048, activation="relu", kernel_regularizer='l2'))
  model.add(Dropout(0.3))
  model.add(Dense(2048, activation="relu", kernel_regularizer='l2'))
  model.add(Dropout(0.3))
  model.add(Dense(1024, activation="relu", kernel_regularizer='l2'))
  model.add(Dropout(0.3))
  model.add(Dense(512, activation="relu", kernel_regularizer='l2'))
  model.add(Dropout(0.2))
  model.add(Dense(512, activation="relu", kernel_regularizer='l2'))
  model.add(Dropout(0.1))
  model.add(Dense(256, activation="relu", kernel_regularizer='l2'))
  model.add(Dense(train_generator.num_classes, activation='softmax'))
      
  model.compile(tf.keras.optimizers.Adam(learning_rate=hparams[HP_LR]), loss="categorical_crossentropy", metrics=['accuracy'])
  callback = tf.keras.callbacks.LearningRateScheduler(scheduler)
  history = model.fit(train_generator, validation_data=valid_generator, epochs=EPOCHS, callbacks=[callback])

  class_names = list(test_generator.class_indices.keys())



  return show_matrix(model,test_generator)

def run(run_dir, hparams, session_num):
  with tf.summary.create_file_writer(run_dir).as_default():
    hp.hparams(hparams)  # record the values used in this trial
    accuracy = train_test_model(hparams, session_num)
    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)

session_num = 0

for alpha_lrs in HP_ALPHA.domain.values:
  for lr in HP_LR.domain.values:
    hparams = {
        HP_ALPHA: alpha_lrs,
        HP_LR: lr,
    }
    run_name = "run-%d" % session_num
    print('--- Starting trial: %s' % run_name)
    print({h.name: hparams[h] for h in hparams})
    run('logs/hparam_tuning/' + run_name, hparams, session_num)
    session_num += 1

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs/hparam_tuning

'''
!zip -r /content/Hparam_logs1.zip /content/logs

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(EPOCHS)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

import csv
lr = history.history['lr']
values = [loss, acc, val_loss, val_acc, lr]
with open('resultat_lr_scheduler_reguL2.csv','w',newline='') as csv_file:
  writer = csv.writer(csv_file, delimiter=';')
  writer.writerow(values)

from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sn
import tensorflow as tf
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True

class_names = list(test_generator.class_indices.keys())

test_generator = train_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    batch_size=1,
    shuffle=True,
    class_mode="categorical",
    subset="validation"
)

list(test_generator.class_indices.keys())

def get_loss_acc():
    test_loss, test_acc = model.evaluate(test_generator, verbose=2)
    print("Accuracy : {}".format(test_acc))
    print("Loss : {}".format(test_loss))

def plot_image(predictions_array, true_label, img):
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])

  plt.imshow(img.astype("uint8"))

  predicted_label = np.argmax(predictions_array)
  if predicted_label == true_label:
    color = 'blue'
  else:
    color = 'red'

  plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],
                                100*np.max(predictions_array),
                                class_names[true_label]),
                                color=color)

def plot_value_array(predictions_array, true_label):
  plt.grid(False)
  plt.xticks(range(len(predictions_array)))
  plt.yticks([])
  thisplot = plt.bar(range(len(predictions_array)), predictions_array, color="#777777")
  plt.ylim([0, 1])
  predicted_label = predictions_array.argmax()

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

def show_matrix():

    filenames = test_generator.filenames
    nb_samples = len(test_generator)
    y_prob=[]
    y_act=[]
    test_generator.reset()
    for _ in range(nb_samples):
        X_test, Y_test = test_generator.next()
        y_prob.append(model.predict(X_test))
        y_act.append(Y_test)

    predicted_class = [list(test_generator.class_indices.keys())[i.argmax()] for i in y_prob]
    actual_class = [list(test_generator.class_indices.keys())[i.argmax()] for i in y_act]

    out_df = pd.DataFrame(np.vstack([predicted_class, actual_class]).T, columns=["predicted_class", "actual_class"])
    confusion_matrix = pd.crosstab(out_df["actual_class"], out_df["predicted_class"], rownames=["Actual"], colnames=["Predicted"], normalize="columns")

    sn.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True, fmt='.2%', cmap='Blues')
    plt.show()
    print('test accuracy : {}'.format((np.diagonal(confusion_matrix).sum()/confusion_matrix.sum().sum()*100)))
    

def show_first_picture(col,rows):
    num_rows = rows
    num_cols = col
    num_images = num_rows * num_cols
    plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))
    for i in range(num_images):
        preimg, lbl = test_generator.next()
        img = 127 + preimg[0]
        label = lbl.argmax()
        prediction = model.predict(preimg)[0]
        plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)
        plot_image(prediction, label, img)
        plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)
        plot_value_array(prediction, label)
    plt.tight_layout()
    plt.show()

show_matrix()
show_first_picture(5,8)
'''